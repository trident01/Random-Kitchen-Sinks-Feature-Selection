Prediction of an outcome variable from multiple observed features is a central question in modern computational statistics with numerous applications, and feature selection for prediction is an important way to facilitate model interpretation and can play a critical role in improving modeling for data with high dimensionality. Here we consider the most general model, with considerations for both non-linearity and non-additivity. Building upon the recently proposed ``Random Kitchen Sinks'' approach to efficient non-linear modeling, and using the Fourier basis to implement both additive and non-additive features, we first present a new application of the full group lasso. Expanding on this for efficiency, we construct models with stochastically generated interaction terms, and use the statistical lasso to regularize parameter estimates. We show how to perform variable selection in a manner that identifies the set of features that together best predict the outcome variable, as well as which specific terms interact and propose a connection to the group lasso approach. The approach is implemented in an R package, and we benchmark it on a variety of simulated and empirical datasets, in areas including robot dynamics, tumor diagnosis, and global health, demonstrating improvements over existing methods for prediction and feature selection.
